############################################################################
#
# == Logging properties
#

# Location of pig log file. If blank, a file with a timestamped slug ('pig_1399336559369.log') will be generated in the current working directory.
#
# pig.logfile=


# Log4j configuration file. Set at runtime with the -4 parameter.
# log4jconf=./conf/log4j.properties

# Verbose Output.
# verbose=false

# Omit timestamps on log messages. (default: false)
# brief=false

# Logging level. debug=OFF|ERROR|WARN|INFO|DEBUG (default: INFO)
# debug=INFO

# Roll up warnings across tasks, so that when millions of mappers suddenly cry
aggregate.warning=true

# == Site-specific Properties
#

# Bootstrap file with default statements to execute in every Pig job, similar to
# .bashrc.  If blank, uses the file '.pigbootup' from your home directory; If a
# value is supplied, that file is NOT loaded.  This does not do tilde expansion
# -- you must supply the full path to the file.
# pig.load.default.statements=/home/bob/.pigrc

# Kill all waiting/running MR jobs upon a MR job failure? (default: false) 
# stop.on.failure=false

# Reuse jars across jobs run by the same user? (default: false) If enabled, jars
# are placed in ${pig.user.cache.location}/${user.name}/.pigcache. Since most
# jars change infrequently, this gives a minor speedup.
#
pig.user.cache.enabled=false

# Base path for storing jars cached by the pig.user.cache.enabled feature. (default: /tmp)
# pig.user.cache.location=/tmp

#== Memory impacting properties
# Amount of memory (as fraction of heap) allocated to bags before a spill is forced
# pig.cachedbag.memusage=0.2

# Don't spill bags smaller than this size (bytes). Default: 5000000, or about 5MB.
# pig.spill.size.threshold=5000000

# EXPERIMENTAL: If a file bigger than this size (bytes) is spilled -- thus
# freeing a bunch of ram -- tell the JVM to perform garbage collection.  This
# should help reduce the number of files being spilled, but causes more-frequent
# garbage collection. Default: 40000000 (about 40 MB)
pig.spill.gc.activation.size=40000000

# Fraction of heap available for the reducer to perform a skewed join. A low
# fraction forces Pig to use more reducers, but increases the copying cost. See
# http://pig.apache.org/docs/r0.12.0/perf.html#skewed-joins
#
# pig.skewedjoin.reduce.memusage=0.3


# === SchemaTuple ===
#
# The SchemaTuple feature (PIG-2632) uses a tuple's schema (when known) to
# generate a custom Java class to hold records. Otherwise, tuples are loaded as
# a plain list that is unaware of its contents' schema -- and so each element
# has to be wrapped as a Java object on its own. This can provide more efficient
# CPU utilization, serialization, and most of all memory usage.
#
pig.schematuple=true

############################################################################
#
# Serialization options
#

# Omit empty part files from the output? (default: false)
#
# * false (default): reducers generates an output file, even if output is empty
# * true (recommended): do not generate zero-byte part files
#
# The default behavior of MapReduce is to generate an empty file for no data, so
# Pig follows that. But many small files can cause annoying extra map tasks and
# put load on the HDFS, so consider setting this to 'true'
#
#pig.output.lazy=true

#
# === Tempfile Handling
#

# EXPERIMENTAL: Storage format for temporary files generated by intermediate
# stages of Pig jobs. This can provide significant speed increases for certain
# codecs. Compress temporary files?
pig.tmpfilecompression=true

# Tempfile storage container type.
#
# * tfile (default, recommended): more efficient, but only supports supports gz(gzip) and lzo compression.
pig.tmpfilecompression.storage=tfile

# Codec types for intermediate job files. tfile supports gz(gzip) and lzo;
# seqfile support gz(gzip), lzo, snappy, bzip2
pig.tmpfilecompression.codec=lzo


# ###########################################################################
#
# Execution options
#

# EXPERIMENTAL: Aggregate records in map task before sending to the combiner?
pig.exec.mapPartAgg=true
pig.exec.mapPartAgg.minReduction=10

#
# === Control how many reducers are used.

# Estimate number of reducers naively using a fixed amount of data per
# reducer. Optimally, you have both fewer reducers than available reduce slots,
# and reducers that are neither getting too little data (less than a half-GB or
# so) nor too much data (more than 2-3 times the reducer child process max heap
# size). The default of 1000000000 (about 1GB) is probably low for a production
# cluster -- however it's much worse to set this too high (reducers spill many
# times over in group-sort) than too low (delay waiting for reduce slots).
#
# Be safe and use around 10 MBytes
pig.exec.reducers.bytes.per.reducer=10000000
#
# === Local mode for small jobs

# EXPERIMENTAL: Use local mode for small jobs? If true, jobs with input data
# size smaller than pig.auto.local.input.maxbytes bytes and one or no reducers
# are run in local mode, which is much faster. Note that file paths are still
# interpreted as pig.exectype implies.
#
# * true (recommended): allow local mode for small jobs, which is much faster.
# * false (default): always use pig.exectype.
#
pig.auto.local.enabled=true

#
# Definition of a small job for the pig.auto.local.enabled feature. Only jobs
# with less than this may bytes are candidates to run locally (default:
# 100000000 bytes, about 1GB)
#
# Be safe and use around 100Mbytes
pig.auto.local.input.maxbytes=10000000

#####################################################################
#
# Advanced Site-specific Customizations
#

# Remove intermediate output files?
#
# * true (default, recommended): remove the files
# * false: do NOT remove the files. You must clean them up yourself.
#
# Keeping them is useful for advanced debugging, but can be dangerous -- you
# must clean them up yourself.  Inspect the intermediate outputs with
#
#     LOAD '/path/to/tmp/file' USING org.apache.pig.impl.io.TFileStorage();
#
# (Or ...SequenceFileInterStorage if pig.tmpfilecompression.storage is seqfile)
#
pig.delete.temp.files=true

# In addition to the fs-style commands (rm, ls, etc) Pig can now execute
# SQL-style DDL commands, eg "sql create table pig_test(name string, age int)".
# The only implemented backend is hcat, and luckily that's also the default.
#
# pig.sql.type=hcat

# Path to the hcat executable, for use with pig.sql.type=hcat (default: null)
#
hcat.bin=/usr/local/hcat/bin/hcat

###########################################################################
#
# Overrides for extreme environments
#
# (Most people won't have to adjust these parameters)
#

# MultiQuery optimization is very useful, and so the recommended default is
# true. You may find a that a script fails to compile under MultiQuery. If so,
# disable it at runtime:
#
#     pig -no_multiquery script_that_makes_pig_sad.pig
#
# opt.multiquery=true

# Enable auto parallelism in tez. This should be used by default unless
# you encounter some bug in automatic parallelism. If set to false, use 1 as
# default parallelism
pig.tez.auto.parallelism=true


# Custom temp folder
# pig.temp.dir=/user/root/tmp
# parallelism level for joins
# default_parallel=6
